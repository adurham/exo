import os
import re
from copy import deepcopy

import mlx.core as mx
import psutil
from mlx_lm.models.cache import (
    ArraysCache,
    CacheList,
    KVCache,
    QuantizedKVCache,
    RotatingKVCache,
)
from mlx_lm.tokenizer_utils import TokenizerWrapper

from exo.shared.types.memory import Memory
from exo.shared.types.mlx import KVCacheType
from exo.worker.engines.mlx import Model
from exo.worker.engines.mlx.constants import CACHE_GROUP_SIZE, KV_CACHE_BITS
from exo.worker.runner.bootstrap import logger

import math


# Fraction of device memory above which LRU eviction kicks in.
# Smaller machines need more aggressive eviction.
def _default_memory_threshold() -> float:
    total_gb = psutil.virtual_memory().total / (1024**3)
    if total_gb >= 128:
        return 0.85
    if total_gb >= 64:
        return 0.80
    if total_gb >= 32:
        return 0.75
    return 0.70


_MEMORY_THRESHOLD = float(
    os.environ.get("EXO_MEMORY_THRESHOLD", _default_memory_threshold())
)


class CacheSnapshot:
    """Snapshot of states at a known token position."""

    def __init__(
        self, states: list[RotatingKVCache | ArraysCache | None], token_count: int
    ):
        self.states = states
        self.token_count = token_count


def snapshot_ssm_states(cache: KVCacheType) -> CacheSnapshot:
    states: list[ArraysCache | RotatingKVCache | None] = []
    for c in cache:
        if isinstance(c, (ArraysCache, RotatingKVCache)):
            states.append(deepcopy(c))
        else:
            states.append(None)
    token_count = cache_length(cache)
    return CacheSnapshot(states=states, token_count=token_count)


def _find_nearest_snapshot(
    snapshots: list[CacheSnapshot],
    target_token_count: int,
) -> CacheSnapshot | None:
    best: CacheSnapshot | None = None
    for snap in snapshots:
        if snap.token_count <= target_token_count and (
            best is None or snap.token_count > best.token_count
        ):
            best = snap
    return best


def has_non_kv_caches(cache: KVCacheType) -> bool:
    """Check if a cache contains any ArraysCache (SSM) entries."""
    return any(isinstance(c, (ArraysCache, RotatingKVCache)) for c in cache)

# Deduplication: entries with >90% prefix overlap are merged instead of duplicated
_MIN_OVERLAP_TO_DEDUP = 0.90
# Maximum number of cache entries before forced eviction
_MAX_ENTRIES = int(os.environ.get("EXO_KV_CACHE_MAX_ENTRIES", "4"))
# Mismatches in first N tokens are flagged as likely volatile patterns
_VOLATILE_DETECTION_THRESHOLD = 100


class KVPrefixCache:
    def __init__(self, group: mx.distributed.Group | None):
        self.prompts: list[mx.array] = []  # mx array of tokens (ints)
        self.caches: list[KVCacheType] = []
        self._snapshots: list[list[CacheSnapshot] | None] = []
        self._last_used: list[int] = []  # monotonic counter of last access per entry
        self._access_counts: list[int] = []  # total access count per entry
        self._access_counter: int = 0
        self._group = group

    def clear(self):
        """Clear all cached prompts and caches."""
        self.prompts.clear()
        self.caches.clear()
        self._snapshots.clear()
        self._last_used.clear()
        self._access_counts.clear()

    def add_kv_cache(
        self,
        prompt_tokens: mx.array,
        cache: KVCacheType,
        ssm_snapshots: list[CacheSnapshot] | None = None,
        normalized_tokens: mx.array | None = None,
    ):
        """Add a new cache entry. Deduplicates overlapping entries and evicts if needed."""
        compare_tokens = normalized_tokens if normalized_tokens is not None else prompt_tokens
        new_len = len(compare_tokens)

        # Dedup: if an existing entry shares >90% prefix, update it instead
        for i, cached_prompt in enumerate(self.prompts):
            prefix_len = get_prefix_length(compare_tokens, cached_prompt)
            cached_len = len(cached_prompt)
            overlap = prefix_len / max(cached_len, new_len) if max(cached_len, new_len) > 0 else 0
            if overlap >= _MIN_OVERLAP_TO_DEDUP:
                if new_len >= cached_len:
                    # New entry is longer or same — replace the existing one
                    self.prompts[i] = compare_tokens
                    self.caches[i] = deepcopy(cache)
                    self._snapshots[i] = ssm_snapshots
                    self._access_counter += 1
                    self._last_used[i] = self._access_counter
                    self._access_counts[i] += 1
                    logger.info(
                        f"KV cache dedup-replaced entry {i}: {cached_len} -> {new_len} tokens "
                        f"({overlap:.0%} overlap)"
                    )
                else:
                    # Existing entry is longer — just bump its access
                    self._access_counter += 1
                    self._last_used[i] = self._access_counter
                    self._access_counts[i] += 1
                    logger.info(
                        f"KV cache dedup-skipped: existing entry {i} ({cached_len} tokens) "
                        f"subsumes new ({new_len} tokens, {overlap:.0%} overlap)"
                    )
                return

        self._evict_if_needed()
        # Store normalized tokens for comparison, original tokens for reference
        self.prompts.append(compare_tokens)
        self.caches.append(deepcopy(cache))
        self._snapshots.append(ssm_snapshots)
        self._access_counter += 1
        self._last_used.append(self._access_counter)
        self._access_counts.append(1)
        logger.info(f"KV cache added: {len(prompt_tokens)} tokens (entries={len(self.caches)})")

    def update_kv_cache(
        self,
        index: int,
        prompt_tokens: mx.array,
        cache: KVCacheType,
        snapshots: list[CacheSnapshot] | None,
        restore_pos: int,
        normalized_tokens: mx.array | None = None,
    ):
        """Update an existing cache entry in-place."""
        old_snapshots = self._snapshots[index]
        merged: list[CacheSnapshot] = []
        if old_snapshots:
            merged = [s for s in old_snapshots if s.token_count <= restore_pos]
        if snapshots:
            merged.extend(snapshots)

        # Store normalized tokens for comparison, like add_kv_cache
        self.prompts[index] = normalized_tokens if normalized_tokens is not None else prompt_tokens
        self.caches[index] = deepcopy(cache)
        self._snapshots[index] = merged or None
        self._access_counter += 1
        self._last_used[index] = self._access_counter
        self._access_counts[index] += 1
        logger.info(f"KV cache updated (index {index}): {len(prompt_tokens)} tokens")

    def _get_snapshot(
        self, entry_index: int, target_token_count: int
    ) -> tuple[int, CacheSnapshot | None]:
        if not has_non_kv_caches(self.caches[entry_index]):
            return target_token_count, None

        snapshots = self._snapshots[entry_index]
        if not snapshots:
            return 0, None

        snap = _find_nearest_snapshot(snapshots, target_token_count)
        if snap is not None:
            return snap.token_count, snap

        return 0, None

    def get_kv_cache(
        self,
        model: Model,
        prompt_tokens: mx.array,
        normalized_tokens: mx.array | None = None,
    ) -> tuple[KVCacheType, mx.array, int | None]:
        """Get KV cache for prompt, returning remaining tokens to prefill.

        Returns:
            Tuple of (cache, remaining_tokens, matched_index) where:
            - cache: KV cache to use for generation
            - remaining_tokens: tokens that still need prefilling
            - matched_index: index of the matched entry (None if no match)

        For models with SSM layers (which are ArraysCache in mlx), the cache is trimmed to the
        nearest SSM snapshot position at or before the match point for correctness.
        Same for rotating KV Cache.
        """
        # Use normalized tokens for comparison if available
        compare_tokens = normalized_tokens if normalized_tokens is not None else prompt_tokens
        max_length = len(compare_tokens)

        best_index: int | None = None
        best_length = 0
        is_exact = False

        # Find best cache match (using normalized tokens)
        for i, cached_prompt in enumerate(self.prompts):
            length = get_prefix_length(compare_tokens, cached_prompt)
            if length >= max_length - 1:
                best_index, best_length = i, length
                is_exact = True
                break
            if length > best_length:
                best_index, best_length = i, length

        if best_index is None:
            return make_kv_cache(model), prompt_tokens, None

        # For exact match: trim to max_length-1 so remaining has the last token
        # For partial match: trim to best_length, remaining has suffix to prefill
        # This ensures stream_generate always has at least one token to start with
        has_ssm = has_non_kv_caches(self.caches[best_index])
        target = (max_length - 1) if is_exact and not has_ssm else best_length
        restore_pos, restore_snap = self._get_snapshot(best_index, target)

        # No usable snapshot — need fresh cache
        if restore_snap is None and has_ssm:
            return make_kv_cache(model), prompt_tokens, None

        prompt_cache = deepcopy(self.caches[best_index])
        cached_length = cache_length(self.caches[best_index])
        tokens_to_trim = cached_length - restore_pos
        if tokens_to_trim > 0:
            trim_cache(prompt_cache, tokens_to_trim, restore_snap)
            # Reset cache offset to match trimmed length
            for c in prompt_cache:
                if hasattr(c, "offset"):
                    c.offset = restore_pos

        self._access_counter += 1
        self._last_used[best_index] = self._access_counter
        # Return remaining from ORIGINAL prompt tokens (for actual computation)
        remaining = prompt_tokens[restore_pos:]

        return prompt_cache, remaining, best_index

    def _entry_value(self, index: int) -> float:
        """Compute eviction value for an entry. Higher = more valuable to keep."""
        token_count = len(self.prompts[index])
        access_count = self._access_counts[index]
        return token_count * math.sqrt(access_count)

    def _evict_if_needed(self):
        """Evict lowest-value entries while memory is high or entry count exceeds max."""
        if len(self.caches) == 0:
            return

        # Evict by value (not pure LRU) until below threshold and max entries
        while len(self.caches) > 0 and (
            len(self.caches) >= _MAX_ENTRIES
            or self.get_memory_used_percentage() > _MEMORY_THRESHOLD
        ):
            # Find lowest-value entry
            min_value = float('inf')
            evict_index = 0
            for i in range(len(self.caches)):
                v = self._entry_value(i)
                if v < min_value:
                    min_value = v
                    evict_index = i

            evicted_tokens = len(self.prompts[evict_index])
            evicted_accesses = self._access_counts[evict_index]
            self.prompts.pop(evict_index)
            self.caches.pop(evict_index)
            self._snapshots.pop(evict_index)
            self._last_used.pop(evict_index)
            self._access_counts.pop(evict_index)
            logger.info(
                f"KV cache evicted entry {evict_index} "
                f"({evicted_tokens} tokens, {evicted_accesses} accesses, value={min_value:.0f}) "
                f"— {len(self.caches)} entries remaining"
            )

    def get_memory_used_percentage(self) -> float:
        local_pressure: float = get_memory_used_percentage()

        if self._group is None:
            return local_pressure

        all_pressure = mx.distributed.all_gather(
            mx.array([local_pressure], dtype=mx.float32),
            group=self._group,
        )
        # .item() evals.
        max_pressure = float(mx.max(all_pressure).item())
        return max_pressure


def trim_cache(
    cache: KVCacheType,
    num_tokens: int,
    snapshot: CacheSnapshot | None = None,
) -> None:
    for i, c in enumerate(cache):
        if isinstance(c, (ArraysCache, RotatingKVCache)):
            if snapshot is not None and snapshot.states[i] is not None:
                cache[i] = deepcopy(snapshot.states[i])  # type: ignore
            else:
                c.state = [None] * len(c.state)  # pyright: ignore[reportUnknownMemberType, reportUnknownArgumentType]
        else:
            c.trim(num_tokens)  # pyright: ignore[reportUnknownMemberType]


def encode_prompt(tokenizer: TokenizerWrapper, prompt: str) -> mx.array:
    """Encode a prompt string to token array.

    For chat-templated prompts (which have their own structure markers like
    <|im_user|>, <|im_middle|>, etc.), we should NOT add BOS/EOS tokens as
    that would corrupt the prompt structure.
    """
    # Chat templates define their own structure - don't add BOS/EOS
    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)
    return mx.array(prompt_tokens)


def _entry_length(
    c: KVCache | RotatingKVCache | QuantizedKVCache | ArraysCache | CacheList,
) -> int:
    # Use .offset attribute which KVCache types have (len() not implemented in older QuantizedKVCache).
    if hasattr(c, "offset"):
        return c.offset
    # For CacheList
    if hasattr(c, "size"):
        return int(c.size())  # type: ignore
    return 0


def cache_length(cache: KVCacheType) -> int:
    """Get the number of tokens in a KV cache."""
    return max(_entry_length(c) for c in cache)


# Volatile patterns stripped from prompts before cache comparison.
# Each pattern should match the full volatile token (e.g., 'cch=abc123;').
_VOLATILE_PATTERNS = [
    re.compile(r'cch=[a-fA-F0-9]+;'),      # Claude Code content-cache hash
    re.compile(r'ccid=[a-fA-F0-9\-]+;'),   # Claude Code conversation ID
]


def normalize_prompt_for_cache(prompt: str) -> str:
    """Strip volatile, semantically-irrelevant tokens from a prompt.

    Removes patterns like 'cch=abc123;' that change every request but
    have no effect on model behavior. This allows the KV prefix cache
    to match prompts that differ only in these volatile sections.
    """
    normalized = prompt
    for pattern in _VOLATILE_PATTERNS:
        normalized = pattern.sub('', normalized)
    return normalized


def get_prefix_length(prompt: mx.array, cached_prompt: mx.array) -> int:
    """Find the length of the common prefix between two token arrays."""
    n = min(int(prompt.shape[0]), int(cached_prompt.shape[0]))
    if n == 0:
        return 0

    equal = mx.equal(prompt[:n], cached_prompt[:n]).astype(mx.int32)
    prefix_mask = mx.cumprod(equal)  # stays 1 until first mismatch, then 0 forever
    prefix_len = int(mx.sum(prefix_mask).item())

    # Log where the mismatch occurs for debugging
    if prefix_len < n and prefix_len < len(prompt) - 1:
        # Use wider context (10 tokens) for early mismatches to aid pattern identification
        is_early = prefix_len < _VOLATILE_DETECTION_THRESHOLD
        ctx_radius = 5 if is_early else 3
        ctx_start = max(0, prefix_len - ctx_radius)
        ctx_end = min(n, prefix_len + ctx_radius + 2)
        prompt_ctx = prompt[ctx_start:ctx_end].tolist()
        cached_ctx = cached_prompt[ctx_start:ctx_end].tolist()

        msg = (
            f"KV prefix mismatch at token {prefix_len}/{n} "
            f"(prompt={len(prompt)}, cached={len(cached_prompt)}). "
            f"Prompt tokens[{ctx_start}:{ctx_end}]: {prompt_ctx}, "
            f"Cached tokens[{ctx_start}:{ctx_end}]: {cached_ctx}"
        )

        if is_early:
            logger.warning(
                f"VOLATILE PATTERN DETECTED — {msg}. "
                f"Mismatch in first {_VOLATILE_DETECTION_THRESHOLD} tokens suggests a volatile "
                f"header (timestamp, hash, session ID). Consider adding to _VOLATILE_PATTERNS."
            )
        else:
            logger.info(msg)

    return prefix_len


def get_available_memory() -> Memory:
    mem: int = psutil.virtual_memory().available
    return Memory.from_bytes(mem)


def get_memory_used_percentage() -> float:
    mem = psutil.virtual_memory()
    # percent is 0-100
    return float(mem.percent / 100)


def make_kv_cache(
    model: Model, max_kv_size: int | None = None, keep: int = 0
) -> KVCacheType:
    assert hasattr(model, "layers")

    if hasattr(model, "make_cache"):
        logger.info("Using MLX LM's make cache")
        return model.make_cache()  # type: ignore

    if max_kv_size is None:
        if KV_CACHE_BITS is None:
            logger.info("Using default KV cache")
            return [KVCache() for _ in model.layers]
        else:
            logger.info("Using quantized KV cache")
            return [
                QuantizedKVCache(group_size=CACHE_GROUP_SIZE, bits=KV_CACHE_BITS)
                for _ in model.layers
            ]
    else:
        logger.info(f"Using rotating KV cache with {max_kv_size=} with {keep=}")
        return [RotatingKVCache(max_size=max_kv_size, keep=keep) for _ in model.layers]
